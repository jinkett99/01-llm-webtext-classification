### Merge in ASTAR dataset, Perform Train-Val-Test split

# Add this in each notebook!
import sys
sys.path.append('../src')  # Add the src directory to the system path

# import dependencies
from file_handling import read_parquet_in_chunks
import pandas as pd

#### Trial read large csv file

# Define the file path
# TEST File Path, original path: "embedding.csv"
file_path = "data/processed_parquet/embedding_subset.parquet"

# read in chunks
embedding_df = read_parquet_in_chunks(file_path, chunksize=10000)

embedding_df.head()

len(embedding_df)

# save to csv
embedding_df.to_csv("data/processed/embedding_subsetv2.csv", index=False)

# The modin.pandas DataFrame is a parallel and distributed drop-in replacement for pandas. 

# find solution if fail to load FULL embedding df.
# Take max 10 websites/records per UEN.

#### Post-processing for embeddings df

cols_to_drop = ['count_0', 'count_1', 'count_2', 'soft_label_category', 'Actual_Category', 'primary_key']
df = embedding_df.drop(columns=cols_to_drop)
df.head()

len(df)

#### SUBSET 10 Random UEN records per Unique UEN
- Create "subset_df" object from embedding_df to reduce computational complexity of modelling process
- Repeated process.

import pandas as pd

def limit_records_per_uen(df, max_records=10):
    """
    Limit the number of records per unique UEN to a maximum of `max_records`.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame with a 'UEN' column.
    max_records (int): The maximum number of records to keep per unique UEN.
    
    Returns:
    pd.DataFrame: A DataFrame with the limited number of records per UEN.
    """
    # Group by 'UEN' and apply a function to limit the number of records
    limited_df = df.groupby('UEN').apply(lambda x: x.head(max_records)).reset_index(drop=True)
    return limited_df

# Apply function
subset_df = limit_records_per_uen(df)

subset_df.head()

# save df to csv
# subset_df.to_csv("datasets/embedding_subset.csv", index=False)

#### Create Train & Test Datasets (with Embeddings)
- Train dataset: No. of records?
- Validation/Test dataset: No. of records?

# import astar train & test df
astar_train = pd.read_csv("data/raw/astar_train.csv")
astar_test = pd.read_csv("data/raw/astar_test.csv")
# embedding_subset = pd.read_csv("datasets/embedding_subset.csv")

def merge_with_embedding(astar_df, embedding_df, output_filename):
    """
    Merges the given astar DataFrame (either train or test) with the embedding DataFrame on 'UEN'.
    Keeps only 'UEN' and 'label' columns from astar_df and all columns except 'UEN' and 'sector' from embedding_df.
    Saves the merged DataFrame as a CSV file in the 'datasets' folder.
    
    Parameters:
    astar_df (pd.DataFrame): The astar DataFrame (either train or test).
    embedding_df (pd.DataFrame): The embedding DataFrame.
    output_filename (str): The name of the output CSV file.
    
    Returns:
    pd.DataFrame: The merged DataFrame.
    """
    
    # Perform the inner join
    merged_df = pd.merge(astar_df, embedding_df, on='UEN', how='inner')
    
    # Drop the 'sector' column from the merged DataFrame
    merged_df = merged_df.drop(columns=['CLUSTER_DEFN'])
    
    # Save the merged DataFrame as a CSV file in the 'datasets' folder
    output_path = f'data/processed/{output_filename}'
    merged_df.to_csv(output_path, index=False)
    
    return merged_df

# Merge and save the train and test DataFrames
merged_train = merge_with_embedding(astar_train, subset_df, 'train.csv')
merged_test = merge_with_embedding(astar_test, subset_df, 'test.csv')

