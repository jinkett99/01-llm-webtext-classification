{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas --index-url=\"/home/packages/Python/3.9.15/simple/\"\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "### Read in BERT embeddings directly\n",
    "Fn to read & concat\n",
    "\n",
    "def read_parquet_blobs_to_dataframe(connection_string, container_name, folder_names):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        # List blobs in the folder\n",
    "        blob_list = container_client.list_blobs(name_starts_with=folder_name)\n",
    "        \n",
    "        # Iterate over blobs in the folder\n",
    "        for blob in blob_list:\n",
    "            # Download blob content\n",
    "            blob_client = container_client.get_blob_client(blob)\n",
    "            blob_data = blob_client.download_blob()\n",
    "            blob_bytes = blob_data.readall()\n",
    "            \n",
    "            # Read Parquet data into a DataFrame\n",
    "            with BytesIO(blob_bytes) as f:\n",
    "                df = pd.read_parquet(f)\n",
    "                dfs.append(df)\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "# connection_string = \"DefaultEndpointsProtocol=https;AccountName=bdamlsa;AccountKey=MdO6w+knH6wyjtpfHjiV0ht6Dl9CVeC/f/XYswzdwXi71Mp4+hfDdAYWKoqsdLkiLl32mdU7hACMluHzMueR4g==;EndpointSuffix=core.windows.net\"\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=bdamlsa;AccountKey=MdO6w+knH6wyjtpfHjiV0ht6Dl9CVeC/f/XYswzdwXi71Mp4+hfDdAYWKoqsdLkiLl32mdU7hACMluHzMueR4g==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"revenue-generating-pred\"\n",
    "folder_names = [\"vectorized_train_data\", \"vectorized_validate_data\", \"vectorized_test_data\", \"vectorized_remaining_data\"]\n",
    "final_df = read_parquet_blobs_to_dataframe(connection_string, container_name, folder_names)\n",
    "\n",
    "final_df.head()\n",
    "\n",
    "len(final_df)\n",
    "\n",
    "final_df[\"UEN\"].nunique()\n",
    "\n",
    "# Save tp csv\n",
    "final_df.to_csv(\"datasets/embedding.csv\", index=False)\n",
    "\n",
    "#### Post-processing for embeddings df\n",
    "\n",
    "cols_to_drop = ['count_0', 'count_1', 'count_2', 'soft_label_category', 'Actual_Category', 'primary_key']\n",
    "df = final_df.drop(columns=cols_to_drop)\n",
    "df.head()\n",
    "\n",
    "#### SUBSET 10 Random UEN records per Unique UEN\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def subset_by_uen(df, uen_column='UEN', max_records=10):\n",
    "    \"\"\"\n",
    "    Subsets the DataFrame by choosing up to a specified number of random records per unique UEN.\n",
    "    If a UEN has fewer than or equal to the specified number of records, it leaves them as is.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    uen_column (str): The name of the UEN column. Default is 'UEN'.\n",
    "    max_records (int): The maximum number of records to keep per unique UEN. Default is 10.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The subsetted DataFrame.\n",
    "    \"\"\"\n",
    "    # Group by the UEN column and apply a lambda function to sample records\n",
    "    subset_df = df.groupby(uen_column).apply(lambda x: x.sample(n=min(len(x), max_records), random_state=1)).reset_index(drop=True)\n",
    "    \n",
    "    return subset_df\n",
    "\n",
    "# Apply function\n",
    "subset_df = subset_by_uen(df)\n",
    "\n",
    "subset_df.head()\n",
    "\n",
    "len(subset_df)\n",
    "\n",
    "# save df to csv\n",
    "subset_df.to_csv(\"embedding_subset.csv\", index=False)\n",
    "\n",
    "# Retrain the model on BERT embeddings*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
